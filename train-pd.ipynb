{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包含四个下采样层和四个上采样层的 U-Net 结构\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # 定义下采样层\n",
    "        self.down = nn.ModuleList()\n",
    "        # shape变化: x -> (N, 512, w/2, h/2)\n",
    "        self.down.append(self._make_conv_block(in_channels, 64))\n",
    "        self.down.append(self._make_conv_block(64, 128))\n",
    "        self.down.append(self._make_conv_block(128, 256))\n",
    "        self.down.append(self._make_conv_block(256, 512))\n",
    "        # 最大池化层，使尺寸下降，下采样\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 定义上采样层\n",
    "        self.up = nn.ModuleList()\n",
    "        # (N, 512, w/2, h/2) -> (N, 256, w, h)\n",
    "        self.up.append(self._make_upconv(512, 256))\n",
    "        # (N, 256, w, h) + (N, 256, w, h) -> (N, 256, w, h)\n",
    "        self.up.append(self._make_conv_block(256 + 256, 256))\n",
    "        # (N, 256, w, h) -> (N, 128, 2w, 2h)\n",
    "        self.up.append(self._make_upconv(256, 128))\n",
    "        # (N, 128, 2w, 2h) + (N, 128, 2w, 2h) -> (N, 128, 2w, 2h)\n",
    "        self.up.append(self._make_conv_block(128 + 128, 128))\n",
    "        # (N, 128, 2w, 2h) -> (N, 64, 4w, 4h)\n",
    "        self.up.append(self._make_upconv(128, 64))\n",
    "        # (N, 64, 4w, 4h) + (N, 64, 4w, 4h) -> (N, 64, 4w, 4h)\n",
    "        self.up.append(self._make_conv_block(64 + 64, 64))\n",
    "        # (N, 64, 4w, 4h) -> (N, 32, 8w, 8h)\n",
    "        self.up.append(self._make_upconv(64, 32))\n",
    "        # (N, 32, 8w, 8h) + (N, 32, 8w, 8h) -> (N, 32, 8w, 8h)\n",
    "        self.up.append(self._make_conv_block(32 + 32, 32))\n",
    "\n",
    "        # 定义输出层\n",
    "        self.out = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for downsample in self.down:\n",
    "            x = downsample(x)\n",
    "            skips.append(x)\n",
    "            x = self.pool(x)\n",
    "        print(f\"x.shape最开始: {x.shape}\")\n",
    "        \n",
    "        skips = list(reversed(skips[:-1]))\n",
    "\n",
    "        for i, upsample in enumerate(self.up):\n",
    "            skip = skips[i]\n",
    "            print(f\"skip.shape: {skip.shape}\")\n",
    "            x = upsample(x)\n",
    "            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            print(f\"x.shape然后{x.shape}\")\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "\n",
    "        # 输出 sigma 平方值\n",
    "        sigma2 = self.out(x)\n",
    "\n",
    "        return sigma2\n",
    "\n",
    "    def _make_conv_block(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        定义卷积块：\n",
    "        不改变特征映射的空间维度\n",
    "        仅改变通道数\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def _make_upconv(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        上采样块：\n",
    "        将尺寸扩大到原来的两倍\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(sigma_sq, v, v_t):\n",
    "    loss = 0.5 * torch.log(sigma_sq) + (v_t - v) ** 2 / (2 * sigma_sq)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.data_files = sorted(os.listdir(self.data_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    \"\"\"\n",
    "    `__getitem__` 方法会在每次加载一个数据时被调用，\n",
    "    它会从指定路径中读取 `.npy` 文件，并将其转换为一个 PyTorch 张量。\n",
    "    然后，使用 PyTorch 提供的 `DataLoader` 类，将数据划分为批次进行训练。\n",
    "    \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        # Load data from file\n",
    "        data = np.load(os.path.join(self.data_path, self.data_files[index]))\n",
    "        # data = data[0:4]\n",
    "        # Convert to tensor\n",
    "        data = torch.from_numpy(data).float()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, batch_size):\n",
    "    # Create data loader\n",
    "    dataset = MyDataset(data_path)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data_loader, num_epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_metric = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch in data_loader:\n",
    "            \n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            inputs = batch[:, :4, :, :]\n",
    "            v = batch[:, 2:4, :, :]\n",
    "            v_t = batch[:, 4:6, :, :]\n",
    "            \n",
    "            # 将梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 前向传递\n",
    "            sigma2 = model(inputs)\n",
    "            # 计算损失和评估指标\n",
    "            loss = custom_loss(inputs, v, v_t)\n",
    "            metric = -loss.item()\n",
    "            # 反向传播和优化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 更新损失和评估指标\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_metric += metric\n",
    "            num_batches += 1\n",
    "\n",
    "        # 计算平均损失和评估指标\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        avg_metric = epoch_metric / num_batches\n",
    "\n",
    "        # 打印训练进度\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Loss={avg_loss:.4f}, Metric={avg_metric:.4f}\")\n",
    "        \n",
    "    torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "------------------------------训练部分------------------------------\n",
    "\"\"\"\n",
    "# 加载数据\n",
    "data_path = '/home/panding/code/UR/data-chair'\n",
    "batch_size = 1\n",
    "\n",
    "my_data_loader = load_data(data_path, batch_size)\n",
    "\n",
    "# 初始化模型、优化器和设备\n",
    "net = UNet(in_channels=4, out_channels=1)\n",
    "Adam_optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 训练循环\n",
    "my_num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape最开始: torch.Size([1, 512, 24, 32])\n",
      "skip.shape: torch.Size([1, 256, 96, 128])\n",
      "x.shape然后torch.Size([1, 256, 96, 128])\n",
      "skip.shape: torch.Size([1, 128, 192, 256])\n",
      "x.shape然后torch.Size([1, 256, 192, 256])\n",
      "skip.shape: torch.Size([1, 64, 384, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [256, 128, 2, 2], expected input[1, 384, 192, 256] to have 256 channels, but got 384 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model\u001b[39m=\u001b[39;49mnet, optimizer\u001b[39m=\u001b[39;49mAdam_optimizer, data_loader\u001b[39m=\u001b[39;49mmy_data_loader, num_epochs\u001b[39m=\u001b[39;49mmy_num_epochs, device\u001b[39m=\u001b[39;49mmy_device)\n",
      "Cell \u001b[0;32mIn[112], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, data_loader, num_epochs, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     19\u001b[0m \u001b[39m# 前向传递\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m sigma2 \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     21\u001b[0m \u001b[39m# 计算损失和评估指标\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss \u001b[39m=\u001b[39m custom_loss(inputs, v, v_t)\n",
      "File \u001b[0;32m~/miniconda3/envs/ur/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[108], line 51\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m skip \u001b[39m=\u001b[39m skips[i]\n\u001b[1;32m     50\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mskip.shape: \u001b[39m\u001b[39m{\u001b[39;00mskip\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m x \u001b[39m=\u001b[39m upsample(x)\n\u001b[1;32m     52\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(x, scale_factor\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx.shape然后\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ur/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ur/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ur/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ur/lib/python3.11/site-packages/torch/nn/modules/conv.py:956\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    951\u001b[0m num_spatial_dims \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    952\u001b[0m output_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_padding(\n\u001b[1;32m    953\u001b[0m     \u001b[39minput\u001b[39m, output_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    954\u001b[0m     num_spatial_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 956\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv_transpose2d(\n\u001b[1;32m    957\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    958\u001b[0m     output_padding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [256, 128, 2, 2], expected input[1, 384, 192, 256] to have 256 channels, but got 384 channels instead"
     ]
    }
   ],
   "source": [
    "train(model=net, optimizer=Adam_optimizer, data_loader=my_data_loader, num_epochs=my_num_epochs, device=my_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
