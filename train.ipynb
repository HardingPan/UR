{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class double_conv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Down sampling\n",
    "        self.conv1 = double_conv(in_channels, 64)\n",
    "        self.conv2 = double_conv(64, 128)\n",
    "        self.conv3 = double_conv(128, 256)\n",
    "        self.conv4 = double_conv(256, 512)\n",
    "\n",
    "        # Up sampling\n",
    "        self.up_conv1 = up_conv(512, 256)\n",
    "        self.conv5 = double_conv(512, 256)\n",
    "        self.up_conv2 = up_conv(256, 128)\n",
    "        self.conv6 = double_conv(256, 128)\n",
    "        self.up_conv3 = up_conv(128, 64)\n",
    "        self.conv7 = double_conv(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Down sampling\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(F.max_pool2d(x1, 2))\n",
    "        x3 = self.conv3(F.max_pool2d(x2, 2))\n",
    "        x4 = self.conv4(F.max_pool2d(x3, 2))\n",
    "\n",
    "        # Up sampling\n",
    "        x = self.up_conv1(x4)\n",
    "        x = torch.cat([x3, x], dim=1)\n",
    "        x = self.conv5(x)\n",
    "        x = self.up_conv2(x)\n",
    "        x = torch.cat([x2, x], dim=1)\n",
    "        x = self.conv6(x)\n",
    "        x = self.up_conv3(x)\n",
    "        x = torch.cat([x1, x], dim=1)\n",
    "        x = self.conv7(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef custom_loss(sigma_sq, v, v_t):\\n    eps = 1e-8\\n    sigma_sq = sigma_sq + eps\\n    loss = 0.5 * torch.log(sigma_sq) + (v_t - v) ** 2 / (2 * sigma_sq)\\n    return torch.mean(loss)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def custom_loss(sigma_sq, v, v_t):\n",
    "    eps = 1e-8\n",
    "    sigma_sq = sigma_sq + eps\n",
    "    loss = 0.5 * torch.log(sigma_sq) + (v_t - v) ** 2 / (2 * sigma_sq)\n",
    "    return torch.mean(loss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(sigma, v, v_t):\n",
    "    eps = 1e-8\n",
    "    sigma2 = sigma ** 2 + eps\n",
    "    # print(f\"sigam2.shape: {sigma2.shape}, v.shape: {v.shape}, v_t.shape: {v_t.shape}\")\n",
    "    loss = torch.log(sigma2) + (v_t - v) ** 2 / sigma2\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.data_files = sorted(os.listdir(self.data_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    \"\"\"\n",
    "    `__getitem__` 方法会在每次加载一个数据时被调用，\n",
    "    它会从指定路径中读取 `.npy` 文件，并将其转换为一个 PyTorch 张量。\n",
    "    然后，使用 PyTorch 提供的 `DataLoader` 类，将数据划分为批次进行训练。\n",
    "    \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        # Load data from file\n",
    "        data = np.load(os.path.join(self.data_path, self.data_files[index]))\n",
    "        # data = data[0:4]\n",
    "        # Convert to tensor\n",
    "        data = torch.from_numpy(data).float()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, batch_size):\n",
    "    # Create data loader\n",
    "    dataset = MyDataset(data_path)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap(inputs, device):\n",
    "    inputs = inputs.cpu().numpy()\n",
    "    N = inputs.shape[0]\n",
    "    inputs_split_list = np.split(inputs, N, axis=0)\n",
    "    inputs_split_list = [np.squeeze(i, axis=0) for i in inputs_split_list]\n",
    "    # print(inputs_split_list[0].shape)\n",
    "    for i in range(N):\n",
    "        img0 = inputs_split_list[i][0]\n",
    "        img1 = inputs_split_list[i][1]\n",
    "        u = inputs_split_list[i][2]\n",
    "        v = inputs_split_list[i][3]\n",
    "\n",
    "        x, y = np.meshgrid(np.arange(img1.shape[1]), np.arange(img1.shape[0]))\n",
    "        x = np.float32(x)\n",
    "        y = np.float32(y)\n",
    "        img0 = cv.remap(img0, x+u, y+v, interpolation = 4)\n",
    "        \n",
    "    inputs_new = np.stack(inputs_split_list, axis = 0)\n",
    "    inputs_new = torch.from_numpy(inputs_new)\n",
    "\n",
    "    return inputs_new.to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data_loader, num_epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_metric = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            inputs = batch[:, :4, :, :]\n",
    "            remap(inputs, device)\n",
    "            v = batch[:, 2:4, :, :]\n",
    "            v_t = batch[:, 4:6, :, :]\n",
    "            inputs = remap(inputs, device)\n",
    "            # 将梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 前向传递\n",
    "            sigma = model(inputs)\n",
    "            # 计算损失和评估指标\n",
    "            loss = custom_loss(sigma, v, v_t)\n",
    "            metric = -loss.item()\n",
    "            # 反向传播和优化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 更新损失和评估指标\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_metric += metric\n",
    "            num_batches += 1\n",
    "\n",
    "        # 计算平均损失和评估指标\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        avg_metric = epoch_metric / num_batches\n",
    "        losses.append(avg_loss)\n",
    "        # 打印训练进度\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Loss={avg_loss:.4f}, Metric={avg_metric:.4f}\")\n",
    "\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Training Loss')\n",
    "    plt.savefig('loss.png')    \n",
    "    torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "------------------------------训练部分------------------------------\n",
    "\"\"\"\n",
    "# 加载数据\n",
    "data_path = '/home/panding/code/UR/data-chair'\n",
    "batch_size = 4\n",
    "\n",
    "my_data_loader = load_data(data_path, batch_size)\n",
    "\n",
    "# 初始化模型、优化器和设备\n",
    "net = UNet(in_channels=4, out_channels=1)\n",
    "Adam_optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 训练循环\n",
    "my_num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model\u001b[39m=\u001b[39;49mnet, optimizer\u001b[39m=\u001b[39;49mAdam_optimizer, data_loader\u001b[39m=\u001b[39;49mmy_data_loader, num_epochs\u001b[39m=\u001b[39;49mmy_num_epochs, device\u001b[39m=\u001b[39;49mmy_device)\n",
      "Cell \u001b[0;32mIn[20], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, data_loader, num_epochs, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m \u001b[39m# 更新损失和评估指标\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     31\u001b[0m epoch_metric \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m metric\n\u001b[1;32m     32\u001b[0m num_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model=net, optimizer=Adam_optimizer, data_loader=my_data_loader, num_epochs=my_num_epochs, device=my_device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ur",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
